{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenthanhhy0108/Tempo-Run/blob/main/Tempo_Run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pnCRQchT27e5"
      },
      "outputs": [],
      "source": [
        "!pip install mtranslate\n",
        "!pip install transformers\n",
        "!pip install googletrans\n",
        "!pip install nltk\n",
        "!pip install pyvi\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trans(sentence):\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  # Lấy ra chỉ số của các từ tiếng Anh trong câu\n",
        "  en_word_indices = [i for i in range(len(words)) if words[i].isalpha() and words[i].isascii()]\n",
        "  if not en_word_indices:\n",
        "        return sentence\n",
        "  result = []\n",
        "  sublist = []\n",
        "  for i in range(len(en_word_indices)):\n",
        "      sublist.append(en_word_indices[i])\n",
        "      if i == len(en_word_indices) - 1 or en_word_indices[i+1] != en_word_indices[i] + 1:\n",
        "          result.append(sublist)\n",
        "          sublist = []\n",
        "  tran_sens = []\n",
        "  for lst in result:\n",
        "      translated_words = translate(' '.join([words[i] for i in lst]), 'vi', 'en')\n",
        "      tran_sen = translated_words.split()\n",
        "      traned = ' '.join(tran_sen)\n",
        "      tran_sens.append(traned)\n",
        "    \n",
        "  # Thay thế các từ tiếng Anh ban đầu bằng các từ đã chuyển đổi\n",
        "  for i in range(len(result)):\n",
        "    \n",
        "    if len(result[i])>1:\n",
        "      k = len(result[i]) -1 \n",
        "      while k != 0:\n",
        "        words[result[i][k]] = ''\n",
        "        k -= 1\n",
        "    words[result[i][0]] = tran_sens[i]\n",
        "  # Ghép lại các từ thành câu\n",
        "  translated_sentence = ' '.join(words).strip()\n",
        "  return translated_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "aht1CYScaDN6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from mtranslate import translate\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Đọc nội dung file vào list\n",
        "with open('/content/data_phase_2.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "\n",
        "with open('vi.txt', 'w', encoding='utf-8') as f:\n",
        "  for line in lines:\n",
        "    new_line0 = line.strip()\n",
        "    new_line1 = trans(new_line0)\n",
        "    new_line1 += '\\n'\n",
        "    f.writelines(new_line1)"
      ],
      "metadata": {
        "id": "Y78DDSIeNZw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/vi.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "with open('overlap.txt', 'w', encoding='utf-8') as f:\n",
        "  for sentence in lines:\n",
        "    if \")\" in sentence:\n",
        "        parts = sentence.split(\")\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \")\".join(parts)\n",
        "    if \"(\" in sentence:\n",
        "        parts = sentence.split(\"(\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \"(\".join(parts)\n",
        "    if \"?\" in sentence:\n",
        "        parts = sentence.split(\"?\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \"?\".join(parts)\n",
        "    if \"!\" in sentence:\n",
        "        parts = sentence.split(\"!\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \"!\".join(parts)\n",
        "    f.write(sentence) "
      ],
      "metadata": {
        "id": "ccWdD2SYN9AS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rcIZupfq13d6"
      },
      "outputs": [],
      "source": [
        "# Đọc nội dung của file vào list\n",
        "with open('/content/overlap.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "# Xoá các kí tự liền nhau giống nhau\n",
        "with open('overlap_end.txt', 'w', encoding='utf-8') as fo:\n",
        "  for i in range(len(lines)):\n",
        "    new_line = ''\n",
        "    for j in range(len(lines[i])):\n",
        "      if j == 0 or lines[i][j] != lines[i][j-1]:\n",
        "        new_line += lines[i][j]\n",
        "    if len(new_line) > 0 and new_line[-1] == lines[i][-1]:\n",
        "      new_line = new_line[:-1]\n",
        "    new_line += '\\n'\n",
        "    fo.write(new_line)\n",
        "\n",
        "import os\n",
        "os.remove('/content/overlap.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/overlap_end.txt', 'r', encoding='utf-8') as input_file, open('vi_low.txt', 'w', encoding='utf-8') as output_file:\n",
        "    for line in input_file:\n",
        "        line = line.lower()  # Chuyển tất cả các kí tự in hoa thành kí tự thường\n",
        "        output_file.write(line)"
      ],
      "metadata": {
        "id": "VxHL2aRtqIr6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_list = []\n",
        "vi_list = []\n",
        "#Predict bằng tiếng Anh\n",
        "with open('/content/data_phase_2.txt', 'r', encoding='utf-8') as f:\n",
        "  lines = f.readlines()\n",
        "for i in range(len(lines)):\n",
        "  if len(lines[i]) <= 10:\n",
        "    eng_list.append(i+1)\n",
        "temp = list(set(eng_list))\n",
        "temp.sort()\n",
        "eng_list = temp\n",
        "\n",
        "#Predict bằng tiếng Việt\n",
        "for i in range(len(lines)):\n",
        "  if i+1 not in eng_list:\n",
        "    vi_list.append(i+1)"
      ],
      "metadata": {
        "id": "JxuWneXFJHri"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "icon_list = []\n",
        "with open('/content/icon.txt', 'r', encoding='utf-8') as f:\n",
        "    icons = f.readlines()\n",
        "for icon in icons:\n",
        "    icon_list.append(icon.strip())\n",
        "\n",
        "with open('/content/vi_low.txt', 'r', encoding='utf-8') as input_file, open('vi_low_noICON0.txt', 'w', encoding='utf-8') as output_file:\n",
        "    lines = input_file.readlines()\n",
        "    for line in lines:\n",
        "        new_line = line.strip()\n",
        "        for icon in icon_list:\n",
        "            if icon in new_line:\n",
        "                new_line = new_line.replace(icon, '')\n",
        "        new_line += '\\n'\n",
        "        output_file.write(new_line)\n",
        "with open('/content/vi_low_noICON0.txt', 'r', encoding='utf-8') as input_file, open('vi_low_noICON.txt', 'w', encoding='utf-8') as output_file:\n",
        "  lines = input_file.readlines()\n",
        "  for line in lines:\n",
        "        new_line = line.strip()\n",
        "        if ':)' in new_line:\n",
        "            new_line = new_line.replace(':)', 'khinh')\n",
        "        if '=)' in new_line:\n",
        "            new_line = new_line.replace('=)', 'khinh')\n",
        "        new_line += '\\n'\n",
        "        output_file.write(new_line)\n",
        "import os\n",
        "os.remove('/content/vi_low_noICON0.txt')"
      ],
      "metadata": {
        "id": "Bul5Xgp-QxG6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/vi_low_noICON.txt', 'r', encoding='utf-8') as input_file, open('output_0.txt', 'w', encoding='utf-8') as output_file:\n",
        "    lines = input_file.readlines()\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        new_words = [words[0]]\n",
        "        for i in range(1, len(words)):\n",
        "            if words[i] != words[i-1]:\n",
        "                new_words.append(words[i])\n",
        "        new_line = ' '.join(new_words)\n",
        "        new_line += '\\n'\n",
        "        output_file.write(new_line)"
      ],
      "metadata": {
        "id": "cYsek_IeaZ6x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_teencode = {}\n",
        "\n",
        "with open('/content/teencode.txt', 'r', encoding='utf-8') as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    k = line.index('\\t')\n",
        "    dict_teencode[line[:k]] = line[k+1:].strip() \n",
        "\n",
        "with open('/content/output_0.txt', 'r', encoding='utf-8') as fi, open('data_WTteencode.txt', 'w', encoding = 'utf-8') as fo:\n",
        "    lines = fi.readlines()\n",
        "    keys = dict_teencode.keys()\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in keys:\n",
        "                words[i] = dict_teencode[words[i]]\n",
        "        new_line = ' '.join(words)\n",
        "        new_line += '\\n'\n",
        "        fo.write(new_line)"
      ],
      "metadata": {
        "id": "ruRDJxfpDG6d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/data_WTteencode.txt', 'r', encoding='utf-8') as input_file, open('data_WTteencode0.txt', 'w', encoding='utf-8') as output_file:\n",
        "    lines = input_file.readlines()\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        new_words = [words[0]]\n",
        "        for i in range(1, len(words)):\n",
        "            if words[i] != words[i-1]:\n",
        "                new_words.append(words[i])\n",
        "        new_line = ' '.join(new_words)\n",
        "        new_line += '\\n'\n",
        "        output_file.write(new_line)"
      ],
      "metadata": {
        "id": "Zfo5Ym1pikZt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/data_WTteencode0.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "with open('output.txt', 'w', encoding='utf-8') as f:\n",
        "  for sentence in lines:\n",
        "    if \".\" in sentence:\n",
        "        parts = sentence.split(\".\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \".\".join(parts)\n",
        "    if \",\" in sentence:\n",
        "        parts = sentence.split(\",\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \",\".join(parts)\n",
        "    if \":\" in sentence:\n",
        "        parts = sentence.split(\":\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \":\".join(parts)\n",
        "    if \"%\" in sentence:\n",
        "        parts = sentence.split(\"%\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \"%\".join(parts)\n",
        "    if \"/\" in sentence:\n",
        "        parts = sentence.split(\"/\")\n",
        "        for i in range(len(parts)-1):\n",
        "            parts[i] = parts[i].rstrip()  # Remove whitespace from the end of each part\n",
        "        sentence = \"/\".join(parts)\n",
        "    f.write(sentence)"
      ],
      "metadata": {
        "id": "3ZgwRm4gg7ML"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output.txt', 'r', encoding='utf-8') as input_file, open('output_new.txt', 'w', encoding='utf-8') as output_file:\n",
        "  lines = input_file.readlines()\n",
        "  for line in lines:\n",
        "    new_line = line.strip()\n",
        "    if new_line[len(new_line) - 1] != '.':\n",
        "      new_line += '.'  # Thêm dấu chấm vào cuối câu\n",
        "    new_line += '\\n'\n",
        "    output_file.write(new_line)"
      ],
      "metadata": {
        "id": "2GNxM8WQBS7i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict bằng tiếng Anh\n",
        "with open('/content/output_new.txt', 'r', encoding='utf-8') as f:\n",
        "  lines = f.readlines()\n",
        "for i in range(len(lines)):\n",
        "  if len(lines[i]) <= 10 and i+1 not in eng_list:\n",
        "    eng_list.append(i+1)\n",
        "vi_list_fix = []\n",
        "for i in range(len(lines)):\n",
        "  if i+1 not in eng_list:\n",
        "    vi_list_fix.append(i+1)"
      ],
      "metadata": {
        "id": "YMYLfp-qlRdk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\", use_fast=False)"
      ],
      "metadata": {
        "id": "6mcVAwESLzkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Tasks:\n",
        "# emoji, emotion, hate, irony, offensive, sentiment\n",
        "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
        "\n",
        "task='sentiment'\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
        "\n",
        "# download label mapping\n",
        "labels=[]\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "    csvreader = csv.reader(html, delimiter='\\t')\n",
        "labels = [row[1] for row in csvreader if len(row) > 1]\n",
        "\n",
        "if os.path.isdir(MODEL):\n",
        "    shutil.rmtree(MODEL)\n",
        "\n",
        "tokenizer_eng = AutoTokenizer.from_pretrained(MODEL)\n",
        "model_eng = AutoModelForSequenceClassification.from_pretrained(MODEL)\n"
      ],
      "metadata": {
        "id": "_ga_6ZUxnoOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import ast\n",
        "with open('/content/output_new.txt', 'r', encoding='utf-8') as f:\n",
        "  lines = [line.strip() for line in f.readlines()]\n",
        "with open('sub_new.txt', 'w', encoding='utf-8') as f:\n",
        "  for i in range(len(lines)):\n",
        "    sentence = lines[i]\n",
        "    if i+1 in vi_list_fix:\n",
        "      input_ids = torch.tensor([tokenizer.encode(sentence)])\n",
        "      with torch.no_grad():\n",
        "        out = model(input_ids)\n",
        "        a = out.logits.softmax(dim=-1).tolist()\n",
        "        max_value = max(a[0])\n",
        "        max_index = a[0].index(max_value)\n",
        "        max_index = str(max_index) # convert to string before writing to file\n",
        "        f.write(max_index + '\\n')\n",
        "    else:\n",
        "      sentence_eng = translate(sentence, \"en\")\n",
        "      encoded_input = tokenizer_eng(sentence_eng, return_tensors='pt')\n",
        "      output = model_eng(**encoded_input)\n",
        "      scores = output[0][0].detach().numpy()\n",
        "      scores = softmax(scores)\n",
        "      ranking = np.argsort(scores)\n",
        "      ranking = ranking[::-1]\n",
        "      prediction = labels[ranking[0]]\n",
        "      f.write(f\"{labels[ranking[0]]}\\n\")"
      ],
      "metadata": {
        "id": "misA6gYloHK9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sub_new.txt', 'r') as input_file, open('submit.txt', 'w') as output_file:\n",
        "    for line in input_file:\n",
        "      if len(line) < 3: \n",
        "        value = int(line.strip())\n",
        "        if value == 0:\n",
        "            output_file.write('negative\\n')\n",
        "        elif value == 1:\n",
        "            output_file.write('positive\\n')\n",
        "        elif value == 2:\n",
        "            output_file.write('neutral\\n')\n",
        "      else:\n",
        "        output_file.write(line)"
      ],
      "metadata": {
        "id": "Is1kDoGiWkOM"
      },
      "execution_count": 20,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqySZ/wXluw7YN3zel8L1b",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}